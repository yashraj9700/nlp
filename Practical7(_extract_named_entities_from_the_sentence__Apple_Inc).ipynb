{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgYHINNHwkGt",
        "outputId": "8f053454-1059-459f-8181-35e356fa2e17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Apple/NNP)\n",
            "  (ORGANIZATION Inc./NNP)\n",
            "  is/VBZ\n",
            "  looking/VBG\n",
            "  at/IN\n",
            "  buying/VBG\n",
            "  U.K./NNP\n",
            "  startup/NN\n",
            "  for/IN\n",
            "  $/$\n",
            "  1/CD\n",
            "  billion/CD\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "#1. Write a Python program using NLTK to extract named entities from the sentence: \"Apple Inc. is lookingat buying U.K. startup for $1 billion.\"\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "# Download required NLTK resources (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Sentence to process\n",
        "sentence = \"Apple Inc. is looking at buying U.K. startup for $1 billion.\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Part of speech tagging\n",
        "tags = pos_tag(tokens)\n",
        "\n",
        "# Named Entity Recognition (NER) chunking\n",
        "named_entities = ne_chunk(tags)\n",
        "\n",
        "# Display the named entities\n",
        "print(named_entities)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Using NLTK, write a function that takes a list of sentences and returns a list of named entities found ineach sentence.\n",
        "def extract_named_entities(sentences):\n",
        "    named_entities = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        tags = pos_tag(words)\n",
        "        tree = ne_chunk(tags)\n",
        "        entities = []\n",
        "        for chunk in tree:\n",
        "            if hasattr(chunk, 'label'):\n",
        "                entities.append((chunk.label(), ' '.join(c[0] for c in chunk)))\n",
        "        named_entities.append(entities)\n",
        "    return named_entities\n",
        "\n",
        "sentences = [\n",
        "    \"Apple Inc. is looking at buying U.K. startup for $1 billion.\",\n",
        "    \"John Smith works at Apple Inc. in California.\"\n",
        "]\n",
        "print(extract_named_entities(sentences))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcsrSE5Xw6Pn",
        "outputId": "554c1c96-6bf8-4a57-d270-2511bb28c448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('PERSON', 'Apple'), ('ORGANIZATION', 'Inc.')], [('PERSON', 'John'), ('PERSON', 'Smith'), ('ORGANIZATION', 'Apple Inc.'), ('GPE', 'California')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a Python program that uses NLTK to extract and display all noun phrases from a given text.\n",
        "from nltk import RegexpParser\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "words = word_tokenize(text)\n",
        "tags = pos_tag(words)\n",
        "\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "parser = RegexpParser(grammar)\n",
        "tree = parser.parse(tags)\n",
        "\n",
        "print(\"Noun Phrases:\")\n",
        "for subtree in tree.subtrees():\n",
        "    if subtree.label() == 'NP':\n",
        "        print(' '.join(word for word, pos in subtree.leaves()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPglXqXqw76O",
        "outputId": "49dcfe1e-b87c-4fe2-8dfe-09868bd0ebd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun Phrases:\n",
            "The quick brown\n",
            "fox\n",
            "the lazy dog\n"
          ]
        }
      ]
    }
  ]
}