{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "part 1"
      ],
      "metadata": {
        "id": "6ezcoAxgrkpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary NLTK libraries\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg, brown, reuters, inaugural\n",
        "from nltk import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.util import bigrams\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Ensure necessary corpora are downloaded\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuxbJEBFpbAj",
        "outputId": "c41f3a39-7918-47cf-f1c2-3857e2c45113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMy2iKRxnnFZ",
        "outputId": "c121b365-d7aa-42e7-c4df-7cba8acda9cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gutenberg corpus file identifiers: ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ],
      "source": [
        "# Q1: List all available file identifiers in the Gutenberg corpus\n",
        "gutenberg_file_ids = gutenberg.fileids()\n",
        "print(\"Gutenberg corpus file identifiers:\", gutenberg_file_ids)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.\tCalculate the avg. word length, avg. sentence length (in words), and lexical diversity for “Moby Dick” by Herman Melville using Gutenberg\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "import numpy as np\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "moby_dick = gutenberg.words('melville-moby_dick.txt')\n",
        "avg_word_length = np.mean([len(word) for word in moby_dick if word.isalpha()])\n",
        "\n",
        "sentences = gutenberg.sents('melville-moby_dick.txt')\n",
        "avg_sentence_length = np.mean([len(sentence) for sentence in sentences])\n",
        "\n",
        "lexical_diversity = len(set(moby_dick)) / len(moby_dick)\n",
        "\n",
        "print(f\"Average word length: {avg_word_length:.2f}\")\n",
        "print(f\"Average sentence length: {avg_sentence_length:.2f}\")\n",
        "print(f\"Lexical diversity: {lexical_diversity:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUjylO-uqunj",
        "outputId": "a378c3ec-17cb-4b42-9498-34a83e080ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average word length: 4.36\n",
            "Average sentence length: 25.93\n",
            "Lexical diversity: 0.07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Brown Corpus find most frequent word in the news category.\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "from collections import Counter\n",
        "\n",
        "# Get words from the news category\n",
        "news_words = brown.words(categories='news')\n",
        "\n",
        "# Find the most frequent word\n",
        "most_frequent_word = Counter(news_words).most_common(10)\n",
        "for word, freq in most_frequent_word:\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgSdgXUtrSEI",
        "outputId": "fd789f38-ece6-4c54-c6fe-f8a928901ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the: 5580\n",
            ",: 5188\n",
            ".: 4030\n",
            "of: 2849\n",
            "and: 2146\n",
            "to: 2116\n",
            "a: 1993\n",
            "in: 1893\n",
            "for: 943\n",
            "The: 806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "part 2"
      ],
      "metadata": {
        "id": "UV3wpk3yrpKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.Use the inaugural address corpus to find the total number of words and the total number of unique words in the inaugural addresses delivered in the 21st century\n",
        "from nltk.corpus import inaugural\n",
        "\n",
        "# Ensure the Inaugural corpus is downloaded\n",
        "nltk.download('inaugural')\n",
        "\n",
        "# Get file IDs for 21st-century addresses (2001 and later)\n",
        "fileids = [fileid for fileid in inaugural.fileids() if int(fileid[:4]) >= 2001]\n",
        "\n",
        "# Extract words from these addresses\n",
        "words = [word.lower() for fileid in fileids for word in inaugural.words(fileid)]\n",
        "\n",
        "# Calculate total words and unique words\n",
        "total_words = len(words)\n",
        "unique_words = len(set(words))\n",
        "\n",
        "print(\"Total words in 21st-century inaugural addresses:\", total_words)\n",
        "print(\"Total unique words in 21st-century inaugural addresses:\", unique_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTGBfnS6royG",
        "outputId": "0b072aca-6da4-43a1-a907-67e63a5d037f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in 21st-century inaugural addresses: 17480\n",
            "Total unique words in 21st-century inaugural addresses: 2773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Write a Python program to find the frequency distribution of the words \"democracy\", \"freedom\", \"liberty\", and \"equality\" in all inaugural addresses using NLTK.\n",
        "import nltk\n",
        "from nltk.corpus import inaugural\n",
        "\n",
        "# Download the inaugural corpus if not already downloaded\n",
        "nltk.download('inaugural')\n",
        "\n",
        "# Define the words to search for\n",
        "target_words = [\"democracy\", \"freedom\", \"liberty\", \"equality\"]\n",
        "\n",
        "# Initialize a dictionary to store frequencies\n",
        "freq_dist = {word: 0 for word in target_words}\n",
        "\n",
        "# Iterate through all inaugural addresses\n",
        "for fileid in inaugural.fileids():\n",
        "    words = inaugural.words(fileid)\n",
        "    for word in words:\n",
        "        if word.lower() in freq_dist:\n",
        "            freq_dist[word.lower()] += 1\n",
        "\n",
        "print(\"Frequency distribution of words:\")\n",
        "for word, freq in freq_dist.items():\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZPJrHkHr1lU",
        "outputId": "96ab2806-87ee-4d2a-8736-75d292d580e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency distribution of words:\n",
            "democracy: 72\n",
            "freedom: 192\n",
            "liberty: 123\n",
            "equality: 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a Python program to display the 5 most common words in the text of \"Sense and Sensibility\" by Jane Austen using the Gutenberg Corpus.\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from collections import Counter\n",
        "\n",
        "# Download the Gutenberg corpus if not already downloaded\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "# Load the text of \"Sense and Sensibility\"\n",
        "text = gutenberg.words('austen-sense.txt')\n",
        "\n",
        "# Filter out non-alphabetic words\n",
        "alphabetic_words = [word.lower() for word in text if word.isalpha()]\n",
        "\n",
        "# Calculate word frequencies\n",
        "word_freq = Counter(alphabetic_words)\n",
        "\n",
        "# Display the 5 most common words\n",
        "most_common_words = word_freq.most_common(5)\n",
        "print(\"5 most common alphabetic words in 'Sense and Sensibility':\")\n",
        "for word, freq in most_common_words:\n",
        "    print(f\"{word}: {freq}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_dUgqGCr3hL",
        "outputId": "b6d91571-a14c-427d-d52d-4f58d25165cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 most common alphabetic words in 'Sense and Sensibility':\n",
            "to: 4116\n",
            "the: 4105\n",
            "of: 3572\n",
            "and: 3491\n",
            "her: 2551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}